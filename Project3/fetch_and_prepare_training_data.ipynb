{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6d585e-4f0b-4d51-9ecb-400729648546",
   "metadata": {},
   "source": [
    "# FETCH TRAINING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65787a-3707-49ea-a132-0ee112555182",
   "metadata": {},
   "source": [
    "This script performs various tasks related to handling and processing image data, including downloading a zip file from an Amazon S3 bucket, extracting the contents of the zip file, creating directories, splitting and moving images into training, validation, and test sets, and uploading files back to S3. The data generated is structured and ready for consumption by our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34b2afa1-a3b8-4481-a4c4-4859fef20f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries:\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import boto3\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9adda4d7-5fa2-453e-a2b5-64c03418ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_zip_from_s3(s3_client, bucket_name, zip_file_key, zip_file_path):\n",
    "    \"\"\"Download zip file from S3.\"\"\"\n",
    "    s3_client.download_file(bucket_name, zip_file_key, zip_file_path)\n",
    "\n",
    "def extract_zip_file(zip_file_path, extract_directory):\n",
    "    \"\"\"Extract zip file to specified directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_directory)\n",
    "\n",
    "def create_directories(directories):\n",
    "    \"\"\"Create directories if they do not exist.\"\"\"\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "def split_and_move_images(category_path, train_dir, val_dir, test_dir, category, sample_fraction):\n",
    "    \"\"\"Split images into training, validation, and test sets, sample them, and move them to respective directories.\"\"\"\n",
    "    images = os.listdir(category_path)\n",
    "    sampled_images = sample_images(images, sample_fraction)\n",
    "    \n",
    "    train_images, temp_images = train_test_split(sampled_images, test_size=0.4, random_state=23)\n",
    "    val_images, test_images = train_test_split(temp_images, test_size=0.5, random_state=23)\n",
    "\n",
    "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, category), exist_ok=True)\n",
    "\n",
    "    for image in train_images:\n",
    "        shutil.move(os.path.join(category_path, image), os.path.join(train_dir, category, image))\n",
    "    for image in val_images:\n",
    "        shutil.move(os.path.join(category_path, image), os.path.join(val_dir, category, image))\n",
    "    for image in test_images:\n",
    "        shutil.move(os.path.join(category_path, image), os.path.join(test_dir, category, image))\n",
    "\n",
    "def sample_images(images, sample_fraction):\n",
    "    \"\"\"Sample a fraction of images randomly.\"\"\"\n",
    "    sample_size = int(len(images) * sample_fraction)\n",
    "    return random.sample(images, sample_size)\n",
    "\n",
    "def zip_directory(directory_path, zip_file_path):\n",
    "    \"\"\"Zip the contents of a directory.\"\"\"\n",
    "    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, directory_path)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "def upload_zip_to_s3(s3_client, bucket_name, zip_file_path, zip_file_key):\n",
    "    \"\"\"Upload zip file to S3.\"\"\"\n",
    "    s3_client.upload_file(zip_file_path, bucket_name, zip_file_key)\n",
    "\n",
    "\n",
    "def zip_directory(directory_to_zip, zip_file_name):\n",
    "    try:\n",
    "        # Create a zip file\n",
    "        shutil.make_archive(zip_file_name.replace('.zip', ''), 'zip', directory_to_zip)\n",
    "        print(f\"Directory {directory_to_zip} has been zipped into {zip_file_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while zipping the directory: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def upload_to_s3(zip_file_name, s3_bucket, s3_object_name):\n",
    "    try:\n",
    "        # Initialize a session using Amazon S3\n",
    "        s3_client = boto3.client('s3')\n",
    "        # Upload the zip file to S3\n",
    "        s3_client.upload_file(zip_file_name, s3_bucket, s3_object_name)\n",
    "        print(f\"File {zip_file_name} has been uploaded to s3://{s3_bucket}/{s3_object_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while uploading the file to S3: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def create_balanced_dataset(base_directory, target_sample_size, output_directory, category_counts):\n",
    "    # Filter out categories with fewer than 1000 images\n",
    "    filtered_categories = {k: v for k, v in category_counts.items() if v >= 1000}\n",
    "    \n",
    "    # Create output directories if they don't exist\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(output_directory, split), exist_ok=True)\n",
    "    \n",
    "    # Dictionary to store selected samples\n",
    "    sampled_images = defaultdict(list)\n",
    "    \n",
    "    # Sample images from each category and split into train, val, test sets\n",
    "    for category, count in filtered_categories.items():\n",
    "        category_path = os.path.join(base_directory, category)\n",
    "        if not os.path.exists(category_path):\n",
    "            continue\n",
    "        images = os.listdir(category_path)\n",
    "        sampled_images[category] = random.sample(images, min(target_sample_size, count))\n",
    "        \n",
    "        train_images, temp_images = train_test_split(sampled_images[category], test_size=0.4, random_state=23)\n",
    "        val_images, test_images = train_test_split(temp_images, test_size=0.5, random_state=23)\n",
    "        \n",
    "        for split, split_images in zip(['train', 'val', 'test'], [train_images, val_images, test_images]):\n",
    "            split_dir = os.path.join(output_directory, split, category)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "            for image in split_images:\n",
    "                shutil.copy(os.path.join(category_path, image), os.path.join(split_dir, image))\n",
    "    \n",
    "    return sampled_images\n",
    "\n",
    "def count_images_in_categories(base_directory):\n",
    "    \"\"\"Count the number of images in each category.\"\"\"\n",
    "    category_counts = {}\n",
    "    for category in os.listdir(base_directory):\n",
    "        category_path = os.path.join(base_directory, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            image_count = len([name for name in os.listdir(category_path) if os.path.isfile(os.path.join(category_path, name))])\n",
    "            category_counts[category] = image_count\n",
    "    return category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a7115c-8148-461f-98d2-ea797fd6f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'awsmlnn-dev'\n",
    "zip_file_key = 'data/Re-PolyVore.zip'\n",
    "zip_file_path = '/tmp/Re-PolyVore.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8729a6e1-b11c-4a61-85fa-cba25d33dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_zip_from_s3(s3_client, bucket_name, zip_file_key, zip_file_path)\n",
    "data_directory = '/home/sagemaker-user/AWSNN/DL/Project/data'\n",
    "extract_directory = os.path.join(data_directory, 'raw_data')\n",
    "extract_zip_file(zip_file_path, extract_directory)\n",
    "raw_data_categories = os.path.join(extract_directory, 'Re-PolyVore')\n",
    "categories = os.listdir(raw_data_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76de30ec-e6f6-48bd-9e90-34ab5437b09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bag',\n",
       " 'bracelet',\n",
       " 'brooch',\n",
       " 'dress',\n",
       " 'earrings',\n",
       " 'eyewear',\n",
       " 'gloves',\n",
       " 'hairwear',\n",
       " 'hats',\n",
       " 'jumpsuit',\n",
       " 'legwear',\n",
       " 'necklace',\n",
       " 'neckwear',\n",
       " 'outwear',\n",
       " 'pants',\n",
       " 'rings',\n",
       " 'shoes',\n",
       " 'skirt',\n",
       " 'top',\n",
       " 'watches']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7afff8e8-db00-427d-8034-586e5608754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_directory = os.path.join(data_directory, 'split_data_sample')\n",
    "train_directory = os.path.join(split_data_directory, 'train')\n",
    "val_directory = os.path.join(split_data_directory, 'validation')\n",
    "test_directory = os.path.join(split_data_directory, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b19fea6-b55e-42d1-acff-befadf92e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts  = count_images_in_categories(raw_data_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd0eaf3-f8a4-4f1b-bcb8-01664deb6824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bag': 12762,\n",
       " 'bracelet': 3105,\n",
       " 'brooch': 598,\n",
       " 'dress': 4488,\n",
       " 'earrings': 3306,\n",
       " 'eyewear': 4009,\n",
       " 'gloves': 233,\n",
       " 'hairwear': 416,\n",
       " 'hats': 1749,\n",
       " 'jumpsuit': 179,\n",
       " 'legwear': 122,\n",
       " 'necklace': 2799,\n",
       " 'neckwear': 714,\n",
       " 'outwear': 6102,\n",
       " 'pants': 5375,\n",
       " 'rings': 1937,\n",
       " 'shoes': 12082,\n",
       " 'skirt': 3185,\n",
       " 'top': 11639,\n",
       " 'watches': 1375}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8695552d-effd-4ed0-a024-8c6e4283b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = raw_data_categories\n",
    "output_directory = '/home/sagemaker-user/AWSNN/DL/Project/data/sample_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d80a7b70-5b64-4b66-aac2-b9f0eff9049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: bag, Sampled Count: 2000\n",
      "Category: bracelet, Sampled Count: 2000\n",
      "Category: dress, Sampled Count: 2000\n",
      "Category: earrings, Sampled Count: 2000\n",
      "Category: eyewear, Sampled Count: 2000\n",
      "Category: hats, Sampled Count: 1749\n",
      "Category: necklace, Sampled Count: 2000\n",
      "Category: outwear, Sampled Count: 2000\n",
      "Category: pants, Sampled Count: 2000\n",
      "Category: rings, Sampled Count: 1937\n",
      "Category: shoes, Sampled Count: 2000\n",
      "Category: skirt, Sampled Count: 2000\n",
      "Category: top, Sampled Count: 2000\n",
      "Category: watches, Sampled Count: 1375\n"
     ]
    }
   ],
   "source": [
    "target_sample_size = 2000\n",
    "sampled_images = create_balanced_dataset(base_directory, target_sample_size, output_directory,category_counts)\n",
    "\n",
    "# Print out the counts of sampled images\n",
    "for category, images in sampled_images.items():\n",
    "    print(f\"Category: {category}, Sampled Count: {len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3afa978-0e50-48f6-b3fe-adaf5495540e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bag': 400,\n",
       " 'bracelet': 400,\n",
       " 'dress': 400,\n",
       " 'earrings': 400,\n",
       " 'eyewear': 400,\n",
       " 'hats': 350,\n",
       " 'necklace': 400,\n",
       " 'outwear': 400,\n",
       " 'pants': 400,\n",
       " 'rings': 388,\n",
       " 'shoes': 400,\n",
       " 'skirt': 400,\n",
       " 'top': 400,\n",
       " 'watches': 275}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_images_in_categories(output_directory + \"/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2095188-ac17-42d0-b51b-d8217e151108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bag': 1200,\n",
       " 'bracelet': 1200,\n",
       " 'dress': 1200,\n",
       " 'earrings': 1200,\n",
       " 'eyewear': 1200,\n",
       " 'hats': 1049,\n",
       " 'necklace': 1200,\n",
       " 'outwear': 1200,\n",
       " 'pants': 1200,\n",
       " 'rings': 1162,\n",
       " 'shoes': 1200,\n",
       " 'skirt': 1200,\n",
       " 'top': 1200,\n",
       " 'watches': 825}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_images_in_categories(output_directory + \"/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14ba65d1-d262-4b4b-ab6d-7ffcdf8e89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_to_zip = '/home/sagemaker-user/AWSNN/DL/Project/data/sample_data'\n",
    "zip_file_name = '/home/sagemaker-user/AWSNN/DL/Project/data/sample_data/data_project.zip'\n",
    "s3_bucket = 'awsmlnn-dev'\n",
    "s3_object_name = 'data/data_project.zip'\n",
    "\n",
    "zip_directory(directory_to_zip, zip_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ada7eed9-a746-4e31-a5d8-0ac33a859e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /home/sagemaker-user/AWSNN/DL/Project/data/testing/data_project.zip has been uploaded to s3://awsmlnn-dev/data/data_project.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_to_s3(zip_file_name, s3_bucket, s3_object_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
